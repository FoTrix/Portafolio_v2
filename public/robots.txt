# Archivo robots.txt para prevenir clonación y scraping no autorizado

User-agent: *
Disallow: /admin/
Disallow: /private/
Disallow: /api/
Disallow: /js/
Disallow: /css/

# Permitir a los motores de búsqueda legítimos
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

# Bloquear bots de scraping conocidos
User-agent: Baiduspider
Disallow: /

User-agent: Yandex
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: HTTrack
Disallow: /

User-agent: wget
Disallow: /

User-agent: curl
Disallow: /

User-agent: Scrapy
Disallow: /

User-agent: PhantomJS
Disallow: /

User-agent: Nutch
Disallow: /

User-agent: ia_archiver
Disallow: /

User-agent: archive.org_bot
Disallow: /

# Establecer límite de frecuencia de rastreo
Crawl-delay: 10

# Sitemap para motores de búsqueda legítimos
Sitemap: https://tudominio.com/sitemap.xml